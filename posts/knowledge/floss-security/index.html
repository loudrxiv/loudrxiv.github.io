<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>FLOSS Security | A loudrxiv - A website for the academically-stunted</title><meta name=keywords content="Knowledge Base,Privacy,Security"><meta name=description content="While source code is critical for user autonomy, it isn&rsquo;t required to evaluate software security or understand run-time behavior.
One of the biggest parts of the Free and Open Source Software definitions is the freedom to study a program and modify it; in other words, access to editable source code. I agree that such access is essential; however, far too many people support source availability for the wrong reasons. One such reason is that source code is necessary to have any degree of transparency into how a piece of software operates, and is therefore necessary to determine if it is at all secure or trustworthy."><meta name=author content="Rohan Kumar"><link rel=canonical href=https://seirdy.one/posts/2022/02/02/floss-security/><link crossorigin=anonymous href=/assets/css/stylesheet.c5277e43fde8b6dabe803dff75b3c935ba15f1a218d18c0bcbaba3460636ce74.css integrity="sha256-xSd+Q/3ottq+gD3/dbPJNboV8aIY0YwLy6ujRgY2znQ=" rel="preload stylesheet" as=style><noscript><link crossorigin=anonymous href=/css/includes/noscript.30127fa68e36d08f5dd7f9d4e717dac42e729b844672afd0fbcacb0d9e508595.css integrity="sha256-MBJ/po420I9d1/nU5xfaxC5ym4RGcq/Q+8rLDZ5QhZU=" rel="preload stylesheet" as=style></noscript><link rel=icon href=https://loudrxiv.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://loudrxiv.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://loudrxiv.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://loudrxiv.github.io/apple-touch-icon.png><link rel=mask-icon href=https://loudrxiv.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><meta property="og:title" content="FLOSS Security"><meta property="og:description" content="While source code is critical for user autonomy, it isn&rsquo;t required to evaluate software security or understand run-time behavior.
One of the biggest parts of the Free and Open Source Software definitions is the freedom to study a program and modify it; in other words, access to editable source code. I agree that such access is essential; however, far too many people support source availability for the wrong reasons. One such reason is that source code is necessary to have any degree of transparency into how a piece of software operates, and is therefore necessary to determine if it is at all secure or trustworthy."><meta property="og:type" content="article"><meta property="og:url" content="https://loudrxiv.github.io/posts/knowledge/floss-security/"><meta property="og:image" content="https://loudrxiv.github.io/waving_snail.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-02-02T23:16:00+00:00"><meta property="article:modified_time" content="2023-02-04T16:21:06-05:00"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://loudrxiv.github.io/waving_snail.png"><meta name=twitter:title content="FLOSS Security"><meta name=twitter:description content="While source code is critical for user autonomy, it isn&rsquo;t required to evaluate software security or understand run-time behavior.
One of the biggest parts of the Free and Open Source Software definitions is the freedom to study a program and modify it; in other words, access to editable source code. I agree that such access is essential; however, far too many people support source availability for the wrong reasons. One such reason is that source code is necessary to have any degree of transparency into how a piece of software operates, and is therefore necessary to determine if it is at all secure or trustworthy."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Categories","item":"https://loudrxiv.github.io/posts/"},{"@type":"ListItem","position":3,"name":"Knowledge Base","item":"https://loudrxiv.github.io/posts/knowledge/"},{"@type":"ListItem","position":4,"name":"FLOSS Security","item":"https://loudrxiv.github.io/posts/knowledge/floss-security/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"FLOSS Security","name":"FLOSS Security","description":"While source code is critical for user autonomy, it isn\u0026rsquo;t required to evaluate software security or understand run-time behavior.\nOne of the biggest parts of the Free and Open Source Software definitions is the freedom to study a program and modify it; in other words, access to editable source code. I agree that such access is essential; however, far too many people support source availability for the wrong reasons. One such reason is that source code is necessary to have any degree of transparency into how a piece of software operates, and is therefore necessary to determine if it is at all secure or trustworthy.","keywords":["Knowledge Base","Privacy","Security"],"articleBody":"While source code is critical for user autonomy, it isn’t required to evaluate software security or understand run-time behavior.\nOne of the biggest parts of the Free and Open Source Software definitions is the freedom to study a program and modify it; in other words, access to editable source code. I agree that such access is essential; however, far too many people support source availability for the wrong reasons. One such reason is that source code is necessary to have any degree of transparency into how a piece of software operates, and is therefore necessary to determine if it is at all secure or trustworthy. Although security through obscurity is certainly not a robust measure, this claim has two issues:\nSource code describes what a program is designed to do; it is unnecessary and insufficient to determine if what it actually does aligns with its intended design. Vulnerability discovery doesn’t require source code. I’d like to expand on these issues, focusing primarily on compiled binaries. Bear in mind that I do not think that source availability is useless from a security perspective (it certainly makes audits easier), and I do think that source availability is required for user freedom. I’m arguing only that source unavailability doesn’t imply insecurity, and source availability doesn’t imply security. It’s possible (and often preferable) to perform security analysis on binaries, without necessarily having source code. In fact, vulnerability discovery doesn’t typically rely upon source code analysis.\nI’ll update this post occasionally as I learn more on the subject. If you like it, check back in a month or two to see if it has something new.\nPS: this stance is not absolute; I concede to several good counter-arguments in a dedicated section!\nHow security fixes work I don’t think anyone seriously claims that software’s security instantly improves the second its source code is published. The argument I’m responding to is that source code is necessary to understand what a program does and how (in)secure it is, and without it we can’t know for sure.\nAssuming a re-write that fundamentally changes a program’s architecture is not an option1, software security typically improves by fixing vulnerabilities via something resembling this process:\nSomeone discovers a vulnerability Developers are informed of the vulnerability Developers reproduce the issue and understand what caused it Developers patch the software to fix the vulnerability Source code is typically helpful (sometimes essential) to Step 3. If someone has completed Step 3, they will require source code to proceed to Step 4. Source code isn’t necessary for Steps 1 and 2; these steps rely upon understanding how a program misbehaves. For that, we use reverse engineering and/or fuzzing.\nReverse engineering Understanding how a program is designed is not the same as understanding what a program does. A reasonable level of one type of understanding does not imply the other.\nSource code2 is essential to describe a program’s high-level, human-comprehensible design; it represents a contract that outlines how a developer expects a program to behave. A compiler or interpreter3 must then translate it into machine instructions. But source code isn’t always easy to map directly to machine instructions because it is part of a complex system:\nCompilers (sometimes even interpreters) can apply optimizations and hardening measures that are difficult to reason about. This is especially true for Just-In-Time compilers that leverage run-time information.\nThe operating system itself may be poorly understood by the developers, and run a program in a way that contradicts a developer’s expectations.\nToolchains, interpreters, and operating systems can have bugs that impact program execution.\nDifferent compilers and compiler flags can offer different security guarantees and mitigations.\nSource code can be deceptive by featuring sneaky obfuscation techniques, sometimes unintentionally. Confusing naming patterns, re-definitions, and vulnerabilities masquerading as innocent bugs have all been well-documented: look up “hypocrite commits” or the Underhanded C Contest for examples.\nAll of the above points apply to each dependency and the underlying operating system, which can impact a program’s behavior.\nFurthermore, all programmers are flawed mortals who don’t always fully understand source code. Everyone who’s done a non-trivial amount of programming is familiar with the feeling of encountering a bug during run-time for which the cause is impossible to find…until they notice it staring them in the face on Line 12. Think of all the bugs that aren’t so easily noticed.\nReading the source code, compiling, and passing tests isn’t sufficient to show us a program’s final behavior. The only way to know what a program does when you run it is to…run it.4\nSpecial builds Almost all programmers are fully aware of their limited ability, which is why most already employ techniques to analyze run-time behavior that don’t depend on source code. For example, developers of several compiled languages5 can build binaries with sanitizers to detect undefined behavior, races, uninitialized reads, etc. that human eyes may have missed when reading source code. While source code is necessary to build these binaries, it isn’t necessary to run them and observe failures.\nDistributing binaries with sanitizers and debug information to testers is a valid way to collect data about a program’s potential security issues.\nDynamic analysis It’s hard to figure out which syscalls and files a large program program needs by reading its source, especially when certain libraries (e.g. the libc implementation/version) can vary. A syscall tracer like strace(1)6 makes the process trivial.\nA personal example: the understanding I gained from strace was necessary for me to write my bubblewrap scripts. These scripts use bubblewrap(1) to sandbox programs with the minimum permissions possible. Analyzing every relevant program and library’s source code would have taken me months, while strace gave me everything I needed to know in an afternoon: analyzing the strace output told me exactly which syscalls to allow and which files to grant access to, without even having to know what language the program was written in. I generated the initial version of the syscall allow-lists with the following command7:\nstrace name-of-program program-args 2\u003e\u00261 \\ | rg '^([a-z_]*)\\(.*' --replace '$1' \\ | sort | uniq This also extends to determining how programs utilize the network: packet sniffers like Wireshark can determine when a program connects to the network, and where it connects.\nThese methods are not flawless. Syscall tracers are only designed to shed light on how a program interacts with the kernel. Kernel interactions tell us plenty (it’s sometimes all we need), but they don’t give the whole story. Furthermore, packet inspection can be made a bit painful by transit encryption8; tracing a program’s execution alongside packet inspection can offer clarity, but this is not easy.\nFor more information, we turn to core dumps, also known as memory dumps. Core dumps share the state of a program during execution or upon crashing, giving us greater visibility into exactly what data a program is processing. Builds containing debugging symbols (e.g. DWARF) have more detailed core dumps. Vendors that release daily snapshots of pre-release builds typically include some symbols to give testers more detail concerning the causes of crashes. Web browsers are a common example: Chromium dev snapshots, Chrome Canary, Firefox Nightly, WebKit Canary builds, etc. all include debug symbols. Until 2019, Minecraft: Bedrock Edition included debug symbols which were used heavily by the modding community.9\nDynamic analysis example: Zoom In 2020, Zoom Video Communications came under scrutiny for marketing its “Zoom” software as a secure, end-to-end encrypted solution for video conferencing. Zoom’s documentation claimed that it used “AES-256” encryption. Without source code, did we have to take the docs at their word?\nThe Citizen Lab didn’t. On 2020-04-03, it published a report revealing critical flaws in Zoom’s encryption. It utilized Wireshark and mitmproxy to analyze networking activity, and inspected core dumps to learn about its encryption implementation. The Citizen Lab’s researchers found that Zoom actually used an incredibly flawed implementation of a weak version of AES-128 (ECB mode), and easily bypassed it.\nSyscall tracing, packet sniffing, and core dumps are great, but they rely on manual execution which might not hit all the desired code paths. Fortunately, there are other forms of analysis available.\nBinary analysis Tracing execution and inspecting memory dumps can be considered forms of reverse engineering, but they only offer a surface-level view of what’s going on. Reverse engineering gets much more interesting when we analyze a binary artifact.\nStatic binary analysis is a powerful way to inspect a program’s underlying design. Decompilation (especially when supplemented with debug symbols) can re-construct a binary’s assembly or source code. Symbol names may look incomprehensible in stripped binaries, and comments will be missing. What’s left is more than enough to decipher control flow to uncover how a program processes data. This process can be tedious, especially if a program uses certain forms of binary obfuscation.\nThe goal doesn’t have to be a complete understanding of a program’s design (incredibly difficult without source code); it’s typically to answer a specific question, fill in a gap left by tracing/fuzzing, or find a well-known property. When developers publish documentation on the security architecture of their closed-source software, reverse engineering tools like decompilers are exactly what you need to verify their honesty (or lack thereof).\nDecompilers are seldom used alone in this context. Instead, they’re typically a component of reverse engineering frameworks that also sport memory analysis, debugging tools, scripting, and sometimes even IDEs. I use the Rizin framework, but Ghidra is also popular. Their documentation should help you get started if you’re interested.\nExample: malware analysis These reverse-engineering techniques—a combination of tracing, packet sniffing, binary analysis, and memory dumps—make up the workings of most modern malware analysis. See this example of a fully-automated analysis of the Zoom Windows installer. It enumerates plenty of information about Zoom without access to its source code: reading unique machine information, anti-VM and anti-reverse-engineering tricks, reading config files, various types of network access, scanning mounted volumes, and more.\nTo try this out yourself, use a sandbox designed for dynamic analysis. Cuckoo is a common and easy-to-use solution, while DRAKVUF is more advanced.\nExtreme example: the truth about Intel ME and AMT The Intel Management Engine (ME) is a mandatory subsystem of all Intel processors (after 2008) with extremely privileged access to the host system. Active Management Technology (AMT) runs atop it on the subset of Intel processors with “vPro” branding. The latter can be disabled and is intended for organizations to remotely manage their inventory (installing software, monitoring, remote power-on/sleep/wake, etc).\nThe fact that Intel ME has such deep access to the host system and the fact that it’s proprietary have both made it the subject of a high degree of scrutiny. Many people (most of whom have little experience in the area) connected these two facts together to allege that the ME is a backdoor, often by confusedly citing functionality of Intel AMT instead of ME. Is it really impossible to know for sure?\nI picked Intel ME+AMT to serve as an extreme example: it shows both the power and limitations of the analysis approaches covered. ME isn’t made of simple executables you can just run in an OS because it sits far below the OS, in what’s sometimes called “Ring -3”.10 Analysis is limited to external monitoring (e.g. by monitoring network activity) and reverse-engineering unpacked partially-obfuscated firmware updates, with help from official documentation. This is slower and harder than analyzing a typical executable or library.\nAnswers are a bit complex and…more boring than what sensationalized headlines would say. Igor Skochinsky (the developer of me-tools) and Nicola Corna (the developer of me_cleaner) presented their analysis of ME in Intel Me: Myths and Reality; Vassilios Ververis thoroughly analyzed AMT in Security Evaluation of Intel’s Active Management Technology. Interestingly, the former pair argues that auditing binary code is preferable to potentially misleading source code: binary analysis allows auditors to “cut the crap” and inspect what software is truly made of. However, this was balanced by a form of binary obfuscation that the pair encountered; I’ll describe it in a moment.\nSimply monitoring network activity and systematically testing all claims made by the documentation allowed Ververis to uncover a host of security issues in Intel AMT. However, no undocumented features have (to my knowledge) been uncovered. The problematic findings revolved around flawed/insecure implementations of documented functionality. In other words: there’s been no evidence of AMT being “a backdoor”, but its security flaws could have had a similar impact. Fortunately, AMT can be disabled. What about ME?\nThis is where some binary analysis comes in. Neither Skochinsky’s ME Secrets presentation nor Intel Me: Myths and Reality seem to enumerate any contradictions with official Intel documentation.\nUnfortunately, some components are poorly understood due to being obfuscated using Huffman compression with unknown dictionaries. Understanding the inner workings of the obfuscated components blurs the line between software reverse-engineering and figuring out how the chips are actually made, the latter of which is nigh-impossible if you don’t have access to a chip lab full of cash. However, black-box analysis does tell us about the capabilities of these components: see page 21 of “ME Secrets”. Thanks to zdctg for clarifying this.\nSkochinsky’s and Corna’s analysis was sufficient to clarify (but not completely contradict) sensationalism claiming that ME can remotely lock any PC (it was a former opt-in feature), can spy on anything the user does (they clarified that access is limited to unblocked parts of the host memory and the integrated GPU, but doesn’t include e.g. the framebuffer), etc.\nWhile claims such as “ME is a black box that can do anything” are misleading, ME not without its share of vulnerabilities. My favorite look at its issues is a presentation by Mark Ermolov and Maxim Goryachy at Black Hat Europe 2017: How to Hack a Turned-Off Computer, or Running Unsigned Code in Intel Management Engine.\nIn short: ME being proprietary doesn’t mean that we can’t find out how (in)secure it is. Binary analysis when paired with runtime inspection can give us a good understanding of what trade-offs we make by using it. While ME has a history of serious vulnerabilities, they’re nowhere near what borderline conspiracy theories claim.11\n(Note: Intel is not alone here. Other chips typically have equivalents, e.g. AMD Secure Technology).\nFuzzing Manual invocation of a program paired with a tracer like strace won’t always exercise all code paths or find edge-cases. Fuzzing helps bridge this gap: it automates the process of causing a program to fail by generating random or malformed data to feed it. Researchers then study failures and failure-conditions to isolate a bug.\nFuzzing doesn’t necessarily depend on access to source code, as it is a black-box technique. Fuzzers like American Fuzzy Loop (AFL) normally use special fuzz-friendly builds, but other fuzzing setups can work with just about any binaries. In fact, some types of fuzz tests (e.g. fuzzing a web API) hardly need any implementation details.\nFuzzing frequently catches bugs that are only apparent by running a program, not by reading source code. Even so, the biggest beneficiaries of fuzzing are open source projects. cURL, OpenSSL, web browsers, text rendering libraries (HarfBuzz, FreeType) and toolchains (GCC, Clang, the official Go toolchain, etc.) are some notable examples.\nI’ve said it before but let me say it again: fuzzing is really the top method to find problems in curl once we’ve fixed all flaws that the static analyzers we use have pointed out. The primary fuzzing for curl is done by OSS-Fuzz, that tirelessly keeps hammering on the most recent curl code.\nDaniel Stenberg | A Google grant for libcurl work If you want to get started with fuzzing, I recommend checking out the quick-start guide for American Fuzzy Loop. Some languages like Go 1.18 also have fuzzing tools available right in the standard library.\nExample: CVE-2022-0185 A recent example of how fuzzing helps spot a vulnerability in an open-source project is CVE-2022-0185: a Linux 0-day found by the Crusaders of Rust a few weeks ago. It was discovered using the syzkaller kernel fuzzer. The process was documented on Will’s Root:\nCVE-2022-0185 - Winning a $31337 Bounty after Pwning Ubuntu and Escaping Google’s KCTF Containers by willsroot\nI highly encourage giving it a read; it’s the perfect example of fuzzing with sanitizers to find a vulnerability, reproducing the vulnerability (by writing a tiny C program), then diving into the source code to find and fix the cause, and finally reporting the issue (with a patch!). When source isn’t available, the vendor would assume responsibility for the “find and fix” steps.\nThe fact that some of the most-used pieces of FLOSS in existence have been the biggest beneficiaries of source-agnostic approaches to vulnerability analysis should be quite revealing. The source code to these projects has received attention from millions of eyes, yet they still invest in fuzzing infrastructure and vulnerability-hunters prefer analyzing artifacts over inspecting the source.\nGood counter-arguments I readily concede to several points in favor of source availability from a security perspective:\nSource code can make analysis easier by supplementing source-independent approaches. The lines between the steps I mentioned in the four-step vulnerability-fixing process are blurry.\nPatching vulnerabilities is important. Source availability makes it possible for the community, package maintainers, or reporters of a vulnerability to patch software. Package maintainers often blur the line between “packager” and “contributor” by helping projects migrate away from abandoned/insecure dependencies. One example that comes to mind is the Python 2 to Python 3 transition for projects like Calibre.12 Being able to fix issues independent of upstream support is an important mitigation against user domestication.\nSome developers/vendors don’t distribute binaries that make use of modern toolchain-level exploit mitigations (e.g. PIE, RELRO, stack canaries, automatic variable initialization, CFI, etc.13). In these cases, building software yourself with these mitigations (or delegating it to a distro that enforces them) requires source code availability (or at least some sort of intermediate representation).\nClosed-source software may or may not have builds available that include sanitizers and debug symbols.\nAlthough fuzzing release binaries is possible, fuzzing is much easier to do when source code is available. Vendors of proprietary software seldom release special fuzz-friendly builds, and filtering out false-positives can be quite tedious without understanding high-level design.\nIt is certainly possible to notice a vulnerability in source code. Excluding low-hanging fruit typically caught by static code analysis and peer review, it’s not the main way most vulnerabilities are found nowadays (thanks to X_CLI for reminding me about what source analysis does accomplish.\nSoftware as a Service can be incredibly difficult to analyze, as we typically have little more than the ability to query a server. Servers don’t send core dumps, server-side binaries, or trace logs for analysis. Furthermore, it’s difficult to verify which software a server is running.14 For services that require trusting a server, access to the server-side software is important from both a security and a user-freedom perspective\nMost of this post is written with the assumption that binaries are inspectable and traceable. Binary obfuscation and some forms of content protection/DRM violate this assumption and actually do make analysis more difficult.\nBeyond source code, transparency into the development helps assure users of compliance with good security practices. Viewing VCS history, patch reviews, linter configurations, etc. reveal the standards that code is being held up to, some of which can be related to bug-squashing and security.\nPatience on Matrix also had a great response, which I agree with and adapt below:\nWhether or not the source code is available for software does not change how insecure it is. However, there are good security-related incentives to publish source code.\nDoing so improves vulnerability patchability and future architectural improvement by lowering the barrier to contribution. The fixes that follow can be shared and used by other projects across the field, some of which can in turn be used by the vendor. This isn’t a zero-sum game; a rising tide lifts all boats. It’s generally good practice to assume an attacker has full knowledge of a system instead of relying on security through obscurity. Releasing code provides strong assurance that this assumption is being made. It’s a way for vendors to put their money where their mouth is. Both Patience and Drew Vault argue that given the above points, a project whose goal is maximum security would release code. Strictly speaking, I agree. Good intentions don’t imply good results, but they can supplement good results to provide some trust in a project’s future.\nConclusion I’ve gone over some examples of how analyzing a software’s security properties need not depend on source code, and vulnerability discovery in both FLOSS and in proprietary software uses source-agnostic techniques. Dynamic and static black-box techniques are powerful tools that work well from user-space (Zoom) to kernel-space (Linux) to low-level components like Intel ME+AMT. Source code enables the vulnerability-fixing process but has limited utility for the evaluation/discovery process.\nDon’t assume software is safer than proprietary alternatives just because its source is visible; come to a conclusion after analyzing both. There are lots of great reasons to switch from macOS or Windows to Linux (it’s been my main OS for years), but security is low on that list.\nAll other things being mostly equal, FLOSS is obviously preferable from a security perspective; I listed some reasons why in the counter-arguments section. Unfortunately, being helpful is not the same as being necessary. All I argue is that source unavailability does not imply insecurity, and source availability does not imply security. Analysis approaches that don’t rely on source are typically the most powerful, and can be applied to both source-available and source-unavailable software. Plenty of proprietary software is more secure than FLOSS alternatives; few would argue that the sandboxing employed by Google Chrome or Microsoft Edge is more vulnerable than Pale Moon or most WebKitGTK-based browsers, for instance.\nReleasing source code is just one thing vendors can do to improve audits; other options include releasing test builds with debug symbols/sanitizers, publishing docs describing their architecture, and/or just keeping software small and simple. We should evaluate software security through study rather than source model. Support the right things for the right reasons, and help others make informed choices with accurate information. There are enough good reasons to support software freedom; we don’t need to rely on bad ones.\nWriting an alternative or re-implementation doesn’t require access to the original’s source code, as is evidenced by a plethora of clean-room re-implementations of existing software written to circumvent the need to comply with license terms. ↩︎\nIdeally well-documented, non-obfuscated code. ↩︎\nOr a JIT compiler, or a bunch of clockwork, or… ↩︎\nFor completeness, I should add that there is one source-based approach that can verify correctness: formal proofs. Functional programming languages that support dependent types can be provably correct at the source level. Assuming their self-hosted toolchains have similar guarantees, developers using these languages might have to worry less about bugs they couldn’t find in the source code. This can alleviate concerns that their language runtimes can make it hard to reason about low-level behavior. Thanks to Adrian Cochrane for pointing this out. ↩︎\nFor example: C, C++, Objective-C, Go, Fortran, and others can utilize sanitizers from Clang and/or GCC. ↩︎\nThis is probably what people in The Matrix were using to see that iconic digital rain. ↩︎\nThis command only lists syscall names, but I did eventually follow the example of sandbox-app-launcher by allowing certain syscalls (e.g. ioctl) only when invoked with certain parameters. Also, I used ripgrep because I’m more familiar with PCRE-style capture groups. ↩︎\nDecrypting these packets typically involves saving and using key logs, or using endpoints with known pre-master secrets. ↩︎\nI invite any modders who miss these debug symbols to check out the FLOSS Minetest, perhaps with the MineClone2 game. ↩︎\nSee page 127-130 of the Invisible Things Lab’s Quest to the Core slides. Bear in mind that they often refer to AMT running atop ME. ↩︎\nAs an aside: your security isn’t necessarily improved by “disabling” it, since it still runs during the initial boot sequence and does provide some hardening measures of its own (e.g., a TPM). ↩︎\nIn 2017, Calibre’s author actually wanted to stay with Python 2 after its EOL date, and maintain Python 2 himself. Users and package maintainers were quite unhappy with this, as Python 2 would no longer be receiving security fixes after 2020. While official releases of Calibre use a bundled Python interpreter, distro packages typically use the system Python package; Calibre’s popularity and insistence on using Python 2 made it a roadblock to getting rid of the Python 2 package in most distros. What eventually happened was that community members (especially Eli Schwartz and Flaviu Tamas submitted patches to migrate Calibre away from Python 2. Calibre migrated to Python 3 by version 5.0. ↩︎\nLinux distributions’ CFI+ASLR implementations rely executables compiled with CFI+PIE support, and ideally with stack-smashing protectors and no-execute bits. These implementations are flawed (see On the Effectiveness of Full-ASLR on 64-bit Linux and Brad Spengler’s presentation comparing these with PaX’s own implementation). ↩︎\nThe best attempt I know of leverages Trusted Execution Environments, but for limited functionality using an implementation that’s far from bulletproof. ↩︎\n","wordCount":"4165","inLanguage":"en","datePublished":"2022-02-02T23:16:00Z","dateModified":"2023-02-04T16:21:06-05:00","author":{"@type":"Person","name":"Rohan Kumar"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://loudrxiv.github.io/posts/knowledge/floss-security/"},"publisher":{"@type":"Organization","name":"A loudrxiv - A website for the academically-stunted","logo":{"@type":"ImageObject","url":"https://loudrxiv.github.io/favicon.ico"}}}</script></head><body class=dark id=top><script crossorigin=anonymous src=/assets/js/theme.b20f95bb4da41ef90a2610a557a7000b2649a3f47282ec571676da6fc0427200.js integrity="sha256-sg+Vu02kHvkKJhClV6cACyZJo/RyguxXFnbab8BCcgA="></script><header class=header><div id=progressBar></div><nav class=nav><div class=logo><a href=https://loudrxiv.github.io accesskey=h title="PrivSec (Alt + H)"><img src=https://loudrxiv.github.io/privsec.png alt aria-label=logo height=30>PrivSec</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><input id=hamburger-input type=checkbox></input>
<label id=hamburger-menu for=hamburger-input></label><div class=overlay></div><ul id=menu><li><a href=https://loudrxiv.github.io/posts/ title=Categories><span>Categories</span></a></li><li><a href=https://loudrxiv.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://loudrxiv.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://loudrxiv.github.io/resources title=Resources><span>Resources</span></a></li><li><a href=https://tommytran.io/tommy.asc title=PGP><span>PGP</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li><li><a href=https://tommytran.io/tommy.crt title=S/MIME><span>S/MIME</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://loudrxiv.github.io>Home</a>&nbsp;»&nbsp;<a href=https://loudrxiv.github.io/posts/>Categories</a>&nbsp;»&nbsp;<a href=https://loudrxiv.github.io/posts/knowledge/>Knowledge Base</a></div><h1 class=post-title>FLOSS Security</h1><div class=post-meta><span title='2022-02-02 23:16:00 +0000 UTC'>February 2, 2022</span>&nbsp;·&nbsp;20 min&nbsp;·&nbsp;4165 words&nbsp;·&nbsp;Rohan Kumar&nbsp;|&nbsp;<a href=https://github.com/PrivSec-dev/privsec.dev/blob/main/content/posts/knowledge/FLOSS%20Security.md rel="noopener noreferrer">Suggest Changes</a>
&nbsp;|&nbsp;<span>Originally published at&nbsp;<a href=https://seirdy.one/posts/2022/02/02/floss-security/ title=https://seirdy.one/posts/2022/02/02/floss-security/ rel="noopener noreferrer">seirdy.one</a></span></div><div class=post-meta><span title="2023-02-04 16:21:06 -0500 -0500"><i>Last updated on February 4, 2023</i></span></div></header><div class="toc side"><details id=toc><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#how-security-fixes-work aria-label="How security fixes work">How security fixes work</a></li><li><a href=#reverse-engineering aria-label="Reverse engineering">Reverse engineering</a><ul><li><a href=#special-builds aria-label="Special builds">Special builds</a></li><li><a href=#dynamic-analysis aria-label="Dynamic analysis">Dynamic analysis</a><ul><li><a href=#dynamic-analysis-example-zoom aria-label="Dynamic analysis example: Zoom">Dynamic analysis example: Zoom</a></li></ul></li><li><a href=#binary-analysis aria-label="Binary analysis">Binary analysis</a></li><li><a href=#example-malware-analysis aria-label="Example: malware analysis">Example: malware analysis</a></li><li><a href=#extreme-example-the-truth-about-intel-me-and-amt aria-label="Extreme example: the truth about Intel ME and AMT">Extreme example: the truth about Intel ME and AMT</a></li></ul></li><li><a href=#fuzzing aria-label=Fuzzing>Fuzzing</a><ul><li><a href=#example-cve-2022-0185 aria-label="Example: CVE-2022-0185">Example: CVE-2022-0185</a></li></ul></li><li><a href=#good-counter-arguments aria-label="Good counter-arguments">Good counter-arguments</a></li><li><a href=#conclusion aria-label=Conclusion>Conclusion</a></li></ul></div></details></div><div class=post-content><p>While source code is critical for user autonomy, it isn&rsquo;t required to evaluate software security or understand run-time behavior.</p><p>One of the biggest parts of the Free and Open Source Software definitions is the freedom to study a program and modify it; in other words, access to editable source code. I agree that such access is essential; however, far too many people support source availability for the <em>wrong</em> reasons. One such reason is that source code is necessary to have any degree of transparency into how a piece of software operates, and is therefore necessary to determine if it is at all secure or trustworthy. Although security through obscurity is certainly not a robust measure, this claim has two issues:</p><ul><li>Source code describes what a program is designed to do; it is unnecessary and insufficient to determine if what it actually does aligns with its intended design.</li><li>Vulnerability discovery doesn&rsquo;t require source code.</li></ul><p>I&rsquo;d like to expand on these issues, focusing primarily on compiled binaries. Bear in mind that I do not think that source availability is <em>useless</em> from a security perspective (it certainly makes audits easier), and I <em>do</em> think that source availability is required for user freedom. I&rsquo;m arguing only that <strong>source unavailability doesn&rsquo;t imply insecurity</strong>, and <strong>source availability doesn&rsquo;t imply security</strong>. It&rsquo;s possible (and often preferable) to perform security analysis on binaries, without necessarily having source code. In fact, vulnerability discovery doesn&rsquo;t typically rely upon source code analysis.</p><p>I&rsquo;ll update this post occasionally as I learn more on the subject. If you like it, check back in a month or two to see if it has something new.</p><p><em>PS: this stance is not absolute; I concede to several <a href=#good-counter-arguments>good counter-arguments in a dedicated section</a>!</em></p><h2 id=how-security-fixes-work>How security fixes work<a hidden class=anchor aria-hidden=true href=#how-security-fixes-work>#</a></h2><p>I don&rsquo;t think anyone seriously claims that software&rsquo;s security instantly improves the second its source code is published. The argument I&rsquo;m responding to is that source code is necessary to understand what a program does and how (in)secure it is, and without it we can&rsquo;t know for sure.</p><p>Assuming a re-write that fundamentally changes a program&rsquo;s architecture is not an option<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>, software security typically improves by fixing vulnerabilities via something resembling this process:</p><ol><li>Someone discovers a vulnerability</li><li>Developers are informed of the vulnerability</li><li>Developers reproduce the issue and understand what caused it</li><li>Developers patch the software to fix the vulnerability</li></ol><p>Source code is typically helpful (sometimes essential) to Step 3. If someone has completed Step 3, they will require source code to proceed to Step 4. Source code <em>isn&rsquo;t necessary for Steps 1 and 2</em>; these steps rely upon understanding how a program misbehaves. For that, we use <em>reverse engineering</em> and/or <em>fuzzing</em>.</p><h2 id=reverse-engineering>Reverse engineering<a hidden class=anchor aria-hidden=true href=#reverse-engineering>#</a></h2><p>Understanding <em>how a program is designed</em> is not the same as understanding <em>what a program does.</em> A reasonable level of one type of understanding does not imply the other.</p><p>Source code<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup> is essential to describe a program&rsquo;s high-level, human-comprehensible design; it represents a contract that outlines how a developer <em>expects</em> a program to behave. A compiler or interpreter<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup> must then translate it into machine instructions. But source code isn&rsquo;t always easy to map directly to machine instructions because it is part of a complex system:</p><ul><li><p>Compilers (sometimes even interpreters) can apply optimizations and hardening measures that are difficult to reason about. This is especially true for Just-In-Time compilers that leverage run-time information.</p></li><li><p>The operating system itself may be poorly understood by the developers, and run a program in a way that contradicts a developer&rsquo;s expectations.</p></li><li><p>Toolchains, interpreters, and operating systems can have bugs that impact program execution.</p></li><li><p>Different compilers and compiler flags can offer different security guarantees and mitigations.</p></li><li><p>Source code can be deceptive by featuring sneaky obfuscation techniques, sometimes unintentionally. Confusing naming patterns, re-definitions, and vulnerabilities masquerading as innocent bugs have all been well-documented: look up &ldquo;hypocrite commits&rdquo; or the <a href=https://en.wikipedia.org/wiki/Underhanded_C_Contest>Underhanded C Contest</a> for examples.</p></li><li><p>All of the above points apply to each dependency and the underlying operating system, which can impact a program&rsquo;s behavior.</p></li></ul><p>Furthermore, all programmers are flawed mortals who don&rsquo;t always fully understand source code. Everyone who&rsquo;s done a non-trivial amount of programming is familiar with the feeling of encountering a bug during run-time for which the cause is impossible to find&mldr;until they notice it staring them in the face on Line 12. Think of all the bugs that <em>aren&rsquo;t</em> so easily noticed.</p><p>Reading the source code, compiling, and passing tests isn&rsquo;t sufficient to show us a program&rsquo;s final behavior. The only way to know what a program does when you run it is to&mldr;run it.<sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup></p><h3 id=special-builds>Special builds<a hidden class=anchor aria-hidden=true href=#special-builds>#</a></h3><p>Almost all programmers are fully aware of their limited ability, which is why most already employ techniques to analyze run-time behavior that don&rsquo;t depend on source code. For example, developers of several compiled languages<sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup> can build binaries with sanitizers to detect undefined behavior, races, uninitialized reads, etc. that human eyes may have missed when reading source code. While source code is necessary to <em>build</em> these binaries, it isn&rsquo;t necessary to run them and observe failures.</p><p>Distributing binaries with sanitizers and debug information to testers is a valid way to collect data about a program&rsquo;s potential security issues.</p><h3 id=dynamic-analysis>Dynamic analysis<a hidden class=anchor aria-hidden=true href=#dynamic-analysis>#</a></h3><p>It&rsquo;s hard to figure out which syscalls and files a large program program needs by reading its source, especially when certain libraries (e.g. the libc implementation/version) can vary. A syscall tracer like <a href=https://strace.io/><code>strace(1)</code></a><sup id=fnref:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup> makes the process trivial.</p><p>A personal example: the understanding I gained from <code>strace</code> was necessary for me to write <a href=https://sr.ht/~seirdy/bwrap-scripts/>my bubblewrap scripts</a>. These scripts use <a href=https://github.com/containers/bubblewrap><code>bubblewrap(1)</code></a> to sandbox programs with the minimum permissions possible. Analyzing every relevant program and library&rsquo;s source code would have taken me months, while <code>strace</code> gave me everything I needed to know in an afternoon: analyzing the <code>strace</code> output told me exactly which syscalls to allow and which files to grant access to, without even having to know what language the program was written in. I generated the initial version of the syscall allow-lists with the following command<sup id=fnref:7><a href=#fn:7 class=footnote-ref role=doc-noteref>7</a></sup>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>strace name-of-program program-args 2&gt;&amp;1 \
</span></span><span class=line><span class=cl>	| rg &#39;^([a-z_]*)\(.*&#39; --replace &#39;$1&#39; \
</span></span><span class=line><span class=cl>	| sort | uniq
</span></span></code></pre></div><p>This also extends to determining how programs utilize the network: packet sniffers like <a href=https://www.wireshark.org/>Wireshark</a> can determine when a program connects to the network, and where it connects.</p><p>These methods are not flawless. Syscall tracers are only designed to shed light on how a program interacts with the kernel. Kernel interactions tell us plenty (it&rsquo;s sometimes all we need), but they don&rsquo;t give the whole story. Furthermore, packet inspection can be made a bit painful by transit encryption<sup id=fnref:8><a href=#fn:8 class=footnote-ref role=doc-noteref>8</a></sup>; tracing a program&rsquo;s execution alongside packet inspection can offer clarity, but this is not easy.</p><p>For more information, we turn to <a href=https://en.wikipedia.org/wiki/Core_dump><strong>core dumps</strong></a>, also known as memory dumps. Core dumps share the state of a program during execution or upon crashing, giving us greater visibility into exactly what data a program is processing. Builds containing debugging symbols (e.g. <a href=https://dwarfstd.org/>DWARF</a>) have more detailed core dumps. Vendors that release daily snapshots of pre-release builds typically include some symbols to give testers more detail concerning the causes of crashes. Web browsers are a common example: Chromium dev snapshots, Chrome Canary, Firefox Nightly, WebKit Canary builds, etc. all include debug symbols. <a href="https://twitter.com/MisteFr/status/1168597562703716354?s=20">Until 2019</a>, <em>Minecraft: Bedrock Edition</em> included debug symbols which were used heavily by the modding community.<sup id=fnref:9><a href=#fn:9 class=footnote-ref role=doc-noteref>9</a></sup></p><h4 id=dynamic-analysis-example-zoom>Dynamic analysis example: Zoom<a hidden class=anchor aria-hidden=true href=#dynamic-analysis-example-zoom>#</a></h4><p>In 2020, Zoom Video Communications came under scrutiny for marketing its &ldquo;Zoom&rdquo; software as a secure, end-to-end encrypted solution for video conferencing. Zoom&rsquo;s documentation claimed that it used &ldquo;AES-256&rdquo; encryption. Without source code, did we have to take the docs at their word?</p><p><a href=https://citizenlab.ca/>The Citizen Lab</a> didn&rsquo;t. On 2020-04-03, it published <a href=https://citizenlab.ca/2020/04/move-fast-roll-your-own-crypto-a-quick-look-at-the-confidentiality-of-zoom-meetings/>a report</a> revealing critical flaws in Zoom&rsquo;s encryption. It utilized Wireshark and <a href=https://mitmproxy.org/>mitmproxy</a> to analyze networking activity, and inspected core dumps to learn about its encryption implementation. The Citizen Lab&rsquo;s researchers found that Zoom actually used an incredibly flawed implementation of a weak version of AES-128 (ECB mode), and easily bypassed it.</p><p>Syscall tracing, packet sniffing, and core dumps are great, but they rely on manual execution which might not hit all the desired code paths. Fortunately, there are other forms of analysis available.</p><h3 id=binary-analysis>Binary analysis<a hidden class=anchor aria-hidden=true href=#binary-analysis>#</a></h3><p>Tracing execution and inspecting memory dumps can be considered forms of reverse engineering, but they only offer a surface-level view of what&rsquo;s going on. Reverse engineering gets much more interesting when we analyze a binary artifact.</p><p>Static binary analysis is a powerful way to inspect a program&rsquo;s underlying design. Decompilation (especially when supplemented with debug symbols) can re-construct a binary&rsquo;s assembly or source code. Symbol names may look incomprehensible in stripped binaries, and comments will be missing. What&rsquo;s left is more than enough to decipher control flow to uncover how a program processes data. This process can be tedious, especially if a program uses certain forms of binary obfuscation.</p><p>The goal doesn&rsquo;t have to be a complete understanding of a program&rsquo;s design (incredibly difficult without source code); it&rsquo;s typically to answer a specific question, fill in a gap left by tracing/fuzzing, or find a well-known property. When developers publish documentation on the security architecture of their closed-source software, reverse engineering tools like decompilers are exactly what you need to verify their honesty (or lack thereof).</p><p>Decompilers are seldom used alone in this context. Instead, they&rsquo;re typically a component of reverse engineering frameworks that also sport memory analysis, debugging tools, scripting, and sometimes even IDEs. I use <a href=https://rizin.re/>the Rizin framework</a>, but <a href=https://ghidra-sre.org/>Ghidra</a> is also popular. Their documentation should help you get started if you&rsquo;re interested.</p><h3 id=example-malware-analysis>Example: malware analysis<a hidden class=anchor aria-hidden=true href=#example-malware-analysis>#</a></h3><p>These reverse-engineering techniques&mdash;a combination of tracing, packet sniffing, binary analysis, and memory dumps&mdash;make up the workings of most modern malware analysis. See <a href=https://www.hybrid-analysis.com/sample/1ef3b7e9ba5f486afe53fcbd71f69c3f9a01813f35732222f64c0981a0906429/5e428f69c88e9e64c33afe64>this example of a fully-automated analysis of the Zoom Windows installer</a>. It enumerates plenty of information about Zoom without access to its source code: reading unique machine information, anti-VM and anti-reverse-engineering tricks, reading config files, various types of network access, scanning mounted volumes, and more.</p><p>To try this out yourself, use a sandbox designed for dynamic analysis. <a href=https://github.com/cuckoosandbox>Cuckoo</a> is a common and easy-to-use solution, while <a href=https://drakvuf.com/>DRAKVUF</a> is more advanced.</p><h3 id=extreme-example-the-truth-about-intel-me-and-amt>Extreme example: the truth about Intel ME and AMT<a hidden class=anchor aria-hidden=true href=#extreme-example-the-truth-about-intel-me-and-amt>#</a></h3><p>The Intel Management Engine (ME) is a mandatory subsystem of all Intel processors (after 2008) with extremely privileged access to the host system. Active Management Technology (AMT) runs atop it on the subset of Intel processors with &ldquo;vPro&rdquo; branding. The latter can be disabled and is intended for organizations to remotely manage their inventory (installing software, monitoring, remote power-on/sleep/wake, etc).</p><p>The fact that Intel ME has such deep access to the host system and the fact that it&rsquo;s proprietary have both made it the subject of a high degree of scrutiny. Many people (most of whom have little experience in the area) connected these two facts together to allege that the ME is a backdoor, often by confusedly citing functionality of Intel AMT instead of ME. Is it really impossible to know for sure?</p><p>I picked Intel ME+AMT to serve as an extreme example: it shows both the power and limitations of the analysis approaches covered. ME isn&rsquo;t made of simple executables you can just run in an OS because it sits far below the OS, in what&rsquo;s sometimes called &ldquo;Ring -3&rdquo;.<sup id=fnref:10><a href=#fn:10 class=footnote-ref role=doc-noteref>10</a></sup> Analysis is limited to external monitoring (e.g. by monitoring network activity) and reverse-engineering unpacked partially-obfuscated firmware updates, with help from official documentation. This is slower and harder than analyzing a typical executable or library.</p><p>Answers are a bit complex and…more boring than what sensationalized headlines would say. <a href=https://twitter.com/igorskochinsky>Igor Skochinsky</a> (the developer of <a href=https://github.com/skochinsky/me-tools>me-tools</a>) and <a href=https://github.com/corna>Nicola Corna</a> (the developer of <a href=https://github.com/corna/me_cleaner>me_cleaner</a>) presented their analysis of ME in <a href=https://fahrplan.events.ccc.de/congress/2017/Fahrplan/system/event_attachments/attachments/000/003/391/original/Intel_ME_myths_and_reality.pdf>Intel Me: Myths and Reality</a>; Vassilios Ververis thoroughly analyzed AMT in <a href=https://kth.diva-portal.org/smash/get/diva2:508256/FULLTEXT01>Security Evaluation of Intel&rsquo;s Active Management Technology</a>. Interestingly, the former pair argues that auditing binary code is preferable to potentially misleading source code: binary analysis allows auditors to “cut the crap” and inspect what software is truly made of. However, this was balanced by a form of binary obfuscation that the pair encountered; I’ll describe it in a moment.</p><p>Simply monitoring network activity and systematically testing all claims made by the documentation allowed Ververis to uncover a host of security issues in Intel AMT. However, no undocumented features have (to my knowledge) been uncovered. The problematic findings revolved around flawed/insecure implementations of documented functionality. In other words: there&rsquo;s been no evidence of AMT being &ldquo;a backdoor&rdquo;, but its security flaws could have had a similar impact. Fortunately, AMT can be disabled. What about ME?</p><p>This is where some binary analysis comes in. Neither Skochinsky&rsquo;s <a href=https://recon.cx/2014/slides/Recon%202014%20Skochinsky.pdf>ME Secrets</a> presentation nor Intel Me: Myths and Reality seem to enumerate any contradictions with <a href=https://link.springer.com/book/10.1007/978-1-4302-6572-6>official Intel documentation</a>.</p><p>Unfortunately, some components are poorly understood due to being obfuscated using <a href=http://io.netgarage.org/me/>Huffman compression with unknown dictionaries</a>. Understanding the inner workings of the obfuscated components blurs the line between software reverse-engineering and figuring out how the chips are actually made, the latter of which is nigh-impossible if you don&rsquo;t have access to a chip lab full of cash. However, black-box analysis does tell us about the capabilities of these components: see page 21 of &ldquo;ME Secrets&rdquo;. Thanks to zdctg for clarifying this.</p><p>Skochinsky&rsquo;s and Corna&rsquo;s analysis was sufficient to clarify (but not completely contradict) sensationalism claiming that ME can remotely lock any PC (it was a former opt-in feature), can spy on anything the user does (they clarified that access is limited to unblocked parts of the host memory and the integrated GPU, but doesn&rsquo;t include e.g. the framebuffer), etc.</p><p>While claims such as &ldquo;ME is a black box that can do anything&rdquo; are misleading, ME not without its share of vulnerabilities. My favorite look at its issues is a presentation by <a href=https://www.blackhat.com/eu-17/speakers/Mark-Ermolov.html>Mark Ermolov</a> and <a href=https://www.blackhat.com/eu-17/speakers/Maxim-Goryachy.html>Maxim Goryachy</a> at Black Hat Europe 2017: <a href=https://www.blackhat.com/docs/eu-17/materials/eu-17-Goryachy-How-To-Hack-A-Turned-Off-Computer-Or-Running-Unsigned-Code-In-Intel-Management-Engine-wp.pdf>How to Hack a Turned-Off Computer, or Running Unsigned Code in Intel Management Engine</a>.</p><p>In short: ME being proprietary doesn&rsquo;t mean that we can&rsquo;t find out how (in)secure it is. Binary analysis when paired with runtime inspection can give us a good understanding of what trade-offs we make by using it. While ME has a history of serious vulnerabilities, they&rsquo;re nowhere near what <a href=https://web.archive.org/web/20210302072839/themerkle.com/what-is-the-intel-management-engine-backdoor/>borderline conspiracy theories</a> claim.<sup id=fnref:11><a href=#fn:11 class=footnote-ref role=doc-noteref>11</a></sup></p><p>(Note: Intel is not alone here. Other chips typically have equivalents, e.g. AMD Secure Technology).</p><h2 id=fuzzing>Fuzzing<a hidden class=anchor aria-hidden=true href=#fuzzing>#</a></h2><p>Manual invocation of a program paired with a tracer like <code>strace</code> won&rsquo;t always exercise all code paths or find edge-cases. <a href=https://en.wikipedia.org/wiki/Fuzzing>Fuzzing helps bridge this gap</a>: it automates the process of causing a program to fail by generating random or malformed data to feed it. Researchers then study failures and failure-conditions to isolate a bug.</p><p>Fuzzing doesn&rsquo;t necessarily depend on access to source code, as it is a black-box technique. Fuzzers like <a href=https://lcamtuf.coredump.cx/afl/>American Fuzzy Loop (AFL)</a> normally use <a href=#special-builds>special fuzz-friendly builds</a>, but <a href=https://aflplus.plus/docs/binaryonly_fuzzing/>other fuzzing setups</a> can work with just about any binaries. In fact, some types of fuzz tests (e.g. <a href=https://github.com/KissPeter/APIFuzzer/>fuzzing a web API</a>) hardly need any implementation details.</p><p>Fuzzing frequently catches bugs that are only apparent by running a program, not by reading source code. Even so, the biggest beneficiaries of fuzzing are open source projects. <a href=https://github.com/curl/curl-fuzzer>cURL</a>, <a href=https://github.com/openssl/openssl/tree/master/fuzz>OpenSSL</a>, web browsers, text rendering libraries (HarfBuzz, FreeType) and toolchains (GCC, Clang, the official Go toolchain, etc.) are some notable examples.</p><blockquote><p>I&rsquo;ve said it before but let me say it again: fuzzing is really the top method to find problems in curl once we&rsquo;ve fixed all flaws that the static analyzers we use have pointed out. The primary fuzzing for curl is done by OSS-Fuzz, that tirelessly keeps hammering on the most recent curl code.</p></blockquote><ul><li><a href=https://daniel.haxx.se/>Daniel Stenberg</a> | <a href=https://daniel.haxx.se/blog/2020/09/23/a-google-grant-for-libcurl-work/>A Google grant for libcurl work</a></li></ul><p>If you want to get started with fuzzing, I recommend checking out <a href=https://github.com/google/AFL/blob/master/docs/QuickStartGuide.txt>the quick-start guide for American Fuzzy Loop</a>. Some languages like Go 1.18 also have fuzzing tools available right in the standard library.</p><h3 id=example-cve-2022-0185>Example: CVE-2022-0185<a hidden class=anchor aria-hidden=true href=#example-cve-2022-0185>#</a></h3><p>A recent example of how fuzzing helps spot a vulnerability in an open-source project is <a href=https://www.openwall.com/lists/oss-security/2022/01/18/7>CVE-2022-0185</a>: a Linux 0-day found by the Crusaders of Rust a few weeks ago. It was discovered using the <a href=https://github.com/google/syzkaller>syzkaller</a> kernel fuzzer. The process was documented on Will&rsquo;s Root:</p><p><a href=https://www.willsroot.io/2022/01/cve-2022-0185.html>CVE-2022-0185 - Winning a $31337 Bounty after Pwning Ubuntu and Escaping Google&rsquo;s KCTF Containers</a> by <a href=https://willsroot.io>willsroot</a></p><p>I <em>highly</em> encourage giving it a read; it&rsquo;s the perfect example of fuzzing with sanitizers to find a vulnerability, reproducing the vulnerability (by writing a tiny C program), <em>then</em> diving into the source code to find and fix the cause, and finally reporting the issue (with a patch!). When source isn&rsquo;t available, the vendor would assume responsibility for the &ldquo;find and fix&rdquo; steps.</p><p>The fact that some of the most-used pieces of FLOSS in existence have been the biggest beneficiaries of source-agnostic approaches to vulnerability analysis should be quite revealing. The source code to these projects has received attention from millions of eyes, yet they <em>still</em> invest in fuzzing infrastructure and vulnerability-hunters prefer analyzing artifacts over inspecting the source.</p><h2 id=good-counter-arguments>Good counter-arguments<a hidden class=anchor aria-hidden=true href=#good-counter-arguments>#</a></h2><p>I readily concede to several points in favor of source availability from a security perspective:</p><ul><li><p>Source code can make analysis <em>easier</em> by <em>supplementing</em> source-independent approaches. The lines between the steps I mentioned in the <a href=#how-security-fixes-work>four-step vulnerability-fixing process</a> are blurry.</p></li><li><p>Patching vulnerabilities is important. Source availability makes it possible for the community, package maintainers, or reporters of a vulnerability to patch software. Package maintainers often blur the line between &ldquo;packager&rdquo; and &ldquo;contributor&rdquo; by helping projects migrate away from abandoned/insecure dependencies. One example that comes to mind is the Python 2 to Python 3 transition for projects like Calibre.<sup id=fnref:12><a href=#fn:12 class=footnote-ref role=doc-noteref>12</a></sup> Being able to fix issues independent of upstream support is an important mitigation against <a href=https://seirdy.one/posts/2021/01/27/whatsapp-and-the-domestication-of-users/>user domestication</a>.</p></li><li><p>Some developers/vendors don&rsquo;t distribute binaries that make use of modern toolchain-level exploit mitigations (e.g. PIE, RELRO, stack canaries, automatic variable initialization, <a href=https://clang.llvm.org/docs/ControlFlowIntegrity.html>CFI</a>, etc.<sup id=fnref:13><a href=#fn:13 class=footnote-ref role=doc-noteref>13</a></sup>). In these cases, building software yourself with these mitigations (or delegating it to a distro that enforces them) requires source code availability (or at least some sort of intermediate representation).</p></li><li><p>Closed-source software may or may not have builds available that include sanitizers and debug symbols.</p></li><li><p>Although fuzzing release binaries is possible, fuzzing is much easier to do when source code is available. Vendors of proprietary software seldom release special fuzz-friendly builds, and filtering out false-positives can be quite tedious without understanding high-level design.</p></li><li><p>It is certainly possible to notice a vulnerability in source code. Excluding low-hanging fruit typically caught by static code analysis and peer review, it&rsquo;s not the main way most vulnerabilities are found nowadays (thanks to <a href=https://www.broken-by-design.fr/>X_CLI</a> for <a href=https://lemmy.ml/post/167321/comment/117774>reminding me about what source analysis does accomplish</a>.</p></li><li><p>Software as a Service can be incredibly difficult to analyze, as we typically have little more than the ability to query a server. Servers don&rsquo;t send core dumps, server-side binaries, or trace logs for analysis. Furthermore, it&rsquo;s difficult to verify which software a server is running.<sup id=fnref:14><a href=#fn:14 class=footnote-ref role=doc-noteref>14</a></sup> For services that require trusting a server, access to the server-side software is important from both a security and a user-freedom perspective</p></li></ul><p>Most of this post is written with the assumption that binaries are inspectable and traceable. Binary obfuscation and some forms of content protection/DRM violate this assumption and actually do make analysis more difficult.</p><p>Beyond source code, transparency into the development helps assure users of compliance with good security practices. Viewing VCS history, patch reviews, linter configurations, etc. reveal the standards that code is being held up to, some of which can be related to bug-squashing and security.</p><p><a href=https://matrix.to/#/@hypokeimenon:tchncs.de>Patience</a> on Matrix also had a great response, which I agree with and adapt below:</p><p>Whether or not the source code is available for software does not change how insecure it is. However, there are good security-related incentives to publish source code.</p><ul><li>Doing so improves vulnerability patchability and future architectural improvement by lowering the barrier to contribution. The fixes that follow can be <em>shared and used by other projects</em> across the field, some of which can in turn be used by the vendor. This isn&rsquo;t a zero-sum game; a rising tide lifts all boats.</li><li>It&rsquo;s generally good practice to assume an attacker has full knowledge of a system instead of relying on security through obscurity. Releasing code provides strong assurance that this assumption is being made. It&rsquo;s a way for vendors to put their money where their mouth is.</li></ul><p>Both Patience and <a href=https://drewdevault.com/>Drew Vault</a> argue that given the above points, a project whose goal is maximum security would release code. Strictly speaking, I agree. Good intentions don&rsquo;t imply good results, but they can <em>supplement</em> good results to provide some trust in a project&rsquo;s future.</p><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>I&rsquo;ve gone over some examples of how analyzing a software&rsquo;s security properties need not depend on source code, and vulnerability discovery in both FLOSS and in proprietary software uses source-agnostic techniques. Dynamic and static black-box techniques are powerful tools that work well from user-space (Zoom) to kernel-space (Linux) to low-level components like Intel ME+AMT. Source code enables the vulnerability-fixing process but has limited utility for the evaluation/discovery process.</p><p>Don&rsquo;t assume software is safer than proprietary alternatives just because its source is visible; come to a conclusion after analyzing both. There are lots of great reasons to switch from macOS or Windows to Linux (it&rsquo;s been my main OS for years), but <a href=https://madaidans-insecurities.github.io/linux.html>security is low on that list</a>.</p><p>All other things being mostly equal, FLOSS is obviously <em>preferable</em> from a security perspective; I listed some reasons why in the counter-arguments section. Unfortunately, being helpful is not the same as being necessary. All I argue is that source unavailability does not imply insecurity, and source availability does not imply security. Analysis approaches that don&rsquo;t rely on source are typically the most powerful, and can be applied to both source-available and source-unavailable software. Plenty of proprietary software is more secure than FLOSS alternatives; few would argue that the sandboxing employed by Google Chrome or Microsoft Edge is more vulnerable than Pale Moon or most WebKitGTK-based browsers, for instance.</p><p>Releasing source code is just one thing vendors can do to improve audits; other options include releasing test builds with debug symbols/sanitizers, publishing docs describing their architecture, and/or just keeping software small and simple. We should evaluate software security through <em>study</em> rather than source model. Support the right things for the right reasons, and help others make informed choices with accurate information. There are enough good reasons to support software freedom; we don&rsquo;t need to rely on bad ones.</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>Writing an alternative or re-implementation doesn&rsquo;t require access to the original&rsquo;s source code, as is evidenced by a plethora of clean-room re-implementations of existing software written to circumvent the need to comply with license terms.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>Ideally well-documented, non-obfuscated code.&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>Or a JIT compiler, or a <a href=https://en.wikipedia.org/wiki/Analytical_Engine>bunch of clockwork</a>, or&mldr;&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p>For completeness, I should add that there is one source-based approach that can verify correctness: formal proofs. Functional programming languages that <a href=https://en.wikipedia.org/wiki/Dependent_type>support dependent types</a> can be provably correct at the source level. Assuming their self-hosted toolchains have similar guarantees, developers using these languages might have to worry less about bugs they couldn&rsquo;t find in the source code. This can alleviate concerns that their language runtimes can make it hard to reason about low-level behavior. Thanks to <a href=https://adrian.geek.nz/>Adrian Cochrane</a> for pointing this out.&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5><p>For example: C, C++, Objective-C, Go, Fortran, and others can utilize sanitizers from Clang and/or GCC.&#160;<a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:6><p>This is probably what people in <em>The Matrix</em> were using to see that iconic <a href=https://en.wikipedia.org/wiki/Matrix_digital_rain>digital rain</a>.&#160;<a href=#fnref:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:7><p>This command only lists syscall names, but I did eventually follow the example of <a href=https://github.com/Whonix/sandbox-app-launcher>sandbox-app-launcher</a> by allowing certain syscalls (e.g. ioctl) only when invoked with certain parameters. Also, I used <a href=https://github.com/BurntSushi/ripgrep>ripgrep</a> because I&rsquo;m more familiar with PCRE-style capture groups.&#160;<a href=#fnref:7 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:8><p>Decrypting these packets typically involves saving and using key logs, or using endpoints with <a href=https://blog.didierstevens.com/2020/12/14/decrypting-tls-streams-with-wireshark-part-1/>known pre-master secrets</a>.&#160;<a href=#fnref:8 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:9><p>I invite any modders who miss these debug symbols to check out the FLOSS <a href=https://www.minetest.net/>Minetest</a>, perhaps with the <a href=https://content.minetest.net/packages/Wuzzy/mineclone2/>MineClone2</a> game.&#160;<a href=#fnref:9 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:10><p>See page 127-130 of the Invisible Things Lab&rsquo;s <a href=https://invisiblethingslab.com/resources/misc09/Quest%20To%20The%20Core%20%28public%29.pdf>Quest to the Core slides</a>. Bear in mind that they often refer to AMT running atop ME.&#160;<a href=#fnref:10 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:11><p>As an aside: your security isn&rsquo;t necessarily improved by &ldquo;disabling&rdquo; it, since it still runs during the initial boot sequence and does provide some hardening measures of its own (e.g., a TPM).&#160;<a href=#fnref:11 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:12><p>In 2017, Calibre&rsquo;s author actually wanted to stay with Python 2 after its EOL date, and <a href=https://bugs.launchpad.net/calibre/+bug/1714107>maintain Python 2 himself</a>. Users and package maintainers were quite unhappy with this, as Python 2 would no longer be receiving security fixes after 2020. While official releases of Calibre use a bundled Python interpreter, distro packages typically use the system Python package; Calibre&rsquo;s popularity and insistence on using Python 2 made it a roadblock to getting rid of the Python 2 package in most distros. What eventually happened was that community members (especially <a href=https://github.com/eli-schwartz>Eli Schwartz</a> and <a href=https://flaviutamas.com/>Flaviu Tamas</a> submitted patches to migrate Calibre away from Python 2. Calibre migrated to Python 3 by <a href=https://calibre-ebook.com/new-in/fourteen>version 5.0</a>.&#160;<a href=#fnref:12 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:13><p>Linux distributions&rsquo; CFI+ASLR implementations rely executables compiled with CFI+PIE support, and ideally with stack-smashing protectors and no-execute bits. These implementations are flawed (see <a href=https://web.archive.org/web/20211021222659/http://cybersecurity.upv.es/attacks/offset2lib/offset2lib-paper.pdf>On the Effectiveness of Full-ASLR on 64-bit Linux</a> and <a href=https://grsecurity.net/PaX-presentation.pdf>Brad Spengler&rsquo;s presentation comparing these with PaX&rsquo;s own implementation</a>).&#160;<a href=#fnref:13 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:14><p>The <a href=https://signal.org/blog/private-contact-discovery/>best attempt I know of</a> leverages <a href=https://en.wikipedia.org/wiki/Trusted_execution_environment>Trusted Execution Environments</a>, but for limited functionality using an implementation that&rsquo;s <a href=https://en.wikipedia.org/wiki/Software_Guard_Extensions#Attacks>far from bulletproof</a>.&#160;<a href=#fnref:14 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://loudrxiv.github.io/tags/knowledge-base/>Knowledge base</a></li><li><a href=https://loudrxiv.github.io/tags/privacy/>Privacy</a></li><li><a href=https://loudrxiv.github.io/tags/security/>Security</a></li></ul><nav class=paginav><a class=prev href=https://loudrxiv.github.io/posts/knowledge/badness-enumeration/><span class=title>« Prev</span><br><span>Badness Enumeration</span></a>
<a class=next href=https://loudrxiv.github.io/posts/knowledge/multi-factor-authentication/><span class=title>Next »</span><br><span>Multi-factor Authentication</span></a></nav></footer></article></main><footer class=footer><span><a href=https://creativecommons.org/licenses/by-sa/4.0/>CC BY-SA 4.0</a></span>
<span>- Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer">Hugo</a> &
        <a href=https://github.com/Wonderfall/hugo-WonderMod/ rel=noopener>WonderMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script defer crossorigin=anonymous src=/assets/js/papermod.7ea300eda6d3653624a576fbc095ccd8a0c2977756acbe5de4114132a72cc7fa.js integrity="sha256-fqMA7abTZTYkpXb7wJXM2KDCl3dWrL5d5BFBMqcsx/o="></script></body></html>